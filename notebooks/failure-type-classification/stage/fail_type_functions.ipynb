{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Failure type classification \n",
    "\n",
    "This is a POC notebook for analyzing testgrids and generating a report that will identify the tests and dates where 4 different types of failures may have occurred.   \n",
    "\n",
    "Goal: Auto label test grids with the following types of failures:\n",
    "\n",
    "* Infra Flake\n",
    "* Flaky Tests\n",
    "* Install Flake \n",
    "* New Test Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T20:10:30.227730Z",
     "start_time": "2021-01-26T20:10:29.130177Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import convolve2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding path to notebooks consisting of modules to be imported\n",
    "module_path_1 = os.path.abspath(os.path.join(\"../../data-sources/TestGrid\"))\n",
    "module_path_2 = os.path.abspath(os.path.join(\"../background\"))\n",
    "if module_path_1 not in sys.path:\n",
    "    sys.path.append(module_path_1)\n",
    "if module_path_2 not in sys.path:\n",
    "    sys.path.append(module_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.testgrid_EDA import decode_run_length\n",
    "from ipynb.fs.defs.testgrid_flakiness_detection import (\n",
    "    calc_optimal_flakiness_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_infra_flake(data, grid, tab_name, grid_name):\n",
    "    \"\"\"\n",
    "    This function takes a 2d numpy array \"grid\" and uses a diagonal edge detecting\n",
    "    filter to identify time windows in which 'infrastructure flakes' occured.\n",
    "\n",
    "    Returns a list of dates and test indexes\n",
    "    \"\"\"\n",
    "    infra_flakes_found = []\n",
    "\n",
    "    # 2d filter that will have its highest value when convolved with a diagonal pattern.\n",
    "    infra_flake_filter = np.array([[-1, 1], [1, -1]])\n",
    "\n",
    "    # Find the spots on the map where the convolution had its maximum value.\n",
    "    spots = convolve2d(infra_flake_filter, grid, mode=\"valid\")\n",
    "    infra_flakes = np.where(spots == 4)\n",
    "\n",
    "    dates = data[tab_name][grid_name][\"timestamps\"]\n",
    "    infra_flake_dates = np.array(dates)[list([infra_flakes][0][1])]\n",
    "    infra_flake_dates = [\n",
    "        datetime.date.fromtimestamp(x // 1000) for x in infra_flake_dates #issue with x here...what is it referring to?\n",
    "    ]\n",
    "\n",
    "    infra_flake_tests = list([infra_flakes][0][0])\n",
    "\n",
    "    infra_flakes_found = list(zip(infra_flake_dates, infra_flake_tests))\n",
    "\n",
    "    return infra_flakes_found\n",
    "\n",
    "\n",
    "def detect_install_flake(data, grid, tab_name, grid_name):\n",
    "    \"\"\"\n",
    "    If greater than 90% of tests are not run for 2 or more consecutive days,\n",
    "    then we will record this period as as an install flake.\n",
    "    \"\"\"\n",
    "    install_flakes = []\n",
    "\n",
    "    n_rows, n_cols = grid.shape\n",
    "    grid = pd.DataFrame(grid)\n",
    "    not_run_percent = grid.apply(lambda x: sum(x == 0) / n_rows, axis=0)\n",
    "    install_errors = not_run_percent > 0.90\n",
    "    install_error_streaks = run_length_encode(install_errors)\n",
    "\n",
    "    for i in install_error_streaks:\n",
    "        if i[0] is True and i[1] >= 2:\n",
    "            install_flakes.append((i[2] - i[1], i[2]))\n",
    "\n",
    "    dates = data[tab_name][grid_name][\"timestamps\"]\n",
    "    install_flake_dates = np.array(dates)[list([install_flakes][0][0])]\n",
    "    install_flake_dates = [\n",
    "        datetime.date.fromtimestamp(x // 1000) for x in install_flake_dates\n",
    "    ]\n",
    "\n",
    "    return install_flake_dates\n",
    "\n",
    "\n",
    "def detect_new_test_failures(data, grid, tab_name, grid_name):\n",
    "    \"\"\"\n",
    "    If 6 or more consecutive failures occur, then we will record this period\n",
    "    as a new test failure\n",
    "    \"\"\"\n",
    "    grid = pd.DataFrame(grid)\n",
    "    new_test_failures = grid.apply(single_new_test_failure, axis=1)\n",
    "    none_empties = new_test_failures[new_test_failures.apply(lambda x: len(x)) > 0]\n",
    "\n",
    "    dates = data[tab_name][grid_name][\"timestamps\"]\n",
    "\n",
    "    for i, j in enumerate(none_empties):\n",
    "        none_empties[i] = [np.array(dates)[[x[0], x[1]]] for x in j]\n",
    "\n",
    "        none_empties[i] = [\n",
    "            (\n",
    "                datetime.date.fromtimestamp(x[0] // 1000),\n",
    "                datetime.date.fromtimestamp(x[1] // 1000),\n",
    "            )\n",
    "            for x in none_empties[i]\n",
    "        ]\n",
    "\n",
    "    idx = list(none_empties.index)\n",
    "    new_test_failures = [(idx[i], none_empties[i]) for i in range(len(none_empties))]\n",
    "\n",
    "    return new_test_failures\n",
    "\n",
    "\n",
    "def single_new_test_failure(test):\n",
    "\n",
    "    \"\"\"given a test as an array of values, uses run length encoding to\n",
    "    find occurences of 6 or moe consecutive failures for a test.\"\"\"\n",
    "\n",
    "    new_test_failure = []\n",
    "    rle = run_length_encode(test)\n",
    "\n",
    "    #     if rle[-1][0] == 0 and rle[-2][0] == -1 and rle[-2][1] >= 6:\n",
    "    #         new_test_failure.append((rle[-2][2]-rle[-2][1],rle[-2][2]))\n",
    "\n",
    "    if rle[-1][0] == 0 and rle[-2][0] == -1:\n",
    "\n",
    "        for i, j in reversed(list(enumerate(rle[:-2]))):\n",
    "            if j[0] == 1:\n",
    "                break\n",
    "\n",
    "        end_of_grid = rle[i:]\n",
    "\n",
    "        count = 0\n",
    "        for streak in end_of_grid:\n",
    "            if streak[0] == -1:\n",
    "                count += streak[1]\n",
    "\n",
    "        if count >= 6:\n",
    "            new_test_failure.append((end_of_grid[0][2], end_of_grid[-1][2]))\n",
    "\n",
    "    return new_test_failure \n",
    "\n",
    "\n",
    "def detect_flaky_test(data, grid, tab_name, grid_name):\n",
    "    \"\"\"\n",
    "    Apply run calc_optimal_flakiness_score to out grid\n",
    "    \"\"\"\n",
    "\n",
    "    flaky_tests = []\n",
    "    dates = data[tab_name][grid_name][\"timestamps\"]\n",
    "    for i, j in enumerate(grid):\n",
    "\n",
    "        # use the calc_optimal_flakiness_score function imported from testgrid_flakiness_detection notebook\n",
    "        found_flakes = calc_optimal_flakiness_score(grid[i])\n",
    "        if len(found_flakes[1].keys()) > 0:\n",
    "            times = [np.array(dates)[[x[0], x[1]]] for x in found_flakes[1].keys()]\n",
    "            times = [\n",
    "                (\n",
    "                    datetime.date.fromtimestamp(x[0] // 1000),\n",
    "                    datetime.date.fromtimestamp(x[1] // 1000),\n",
    "                )\n",
    "                for x in times\n",
    "            ]\n",
    "\n",
    "            flaky_tests.append((i, found_flakes[1], times))\n",
    "\n",
    "    return flaky_tests\n",
    "\n",
    "\n",
    "def detect_failures(data, grid, tab_name, grid_name):\n",
    "    \"\"\"\n",
    "    This takens in a grid and runs all of our detectors and outputs a report\n",
    "    \"\"\"\n",
    "\n",
    "    failure_report = {}\n",
    "\n",
    "    # use the decode_run_length function imported from TestGrid_EDA notebook\n",
    "    x = np.array(list(pd.DataFrame(grid).statuses.apply(decode_run_length)))\n",
    "\n",
    "    failure_report[\"flaky_tests\"] = detect_flaky_test(data, x, tab_name, grid_name)\n",
    "\n",
    "    x = pd.DataFrame(x).apply(lambda x: [normalize(y) for y in x])\n",
    "    x = np.array(x)\n",
    "\n",
    "    failure_report[\"infra_flake\"] = detect_infra_flake(data, x, tab_name, grid_name)\n",
    "    failure_report[\"install_flake\"] = detect_install_flake(data, x, tab_name, grid_name)\n",
    "    failure_report[\"new_test_failure\"] = detect_new_test_failures(\n",
    "        data, x, tab_name, grid_name\n",
    "    )\n",
    "\n",
    "    return failure_report\n",
    "\n",
    "\n",
    "def print_report(results, tab_name, grid_name):\n",
    "    print(\n",
    "        f\"Failure Report for: \\n\\\n",
    "    {tab_name}/{grid_name}\",\n",
    "        end=\"\\n\\n\",\n",
    "    )\n",
    "\n",
    "    print(\"Flaky Tests:\")\n",
    "    for ft in results[\"flaky_tests\"]:\n",
    "        print(f\"Test number {ft[0]} had flakes at:\")\n",
    "        for i in ft[2]:\n",
    "            print(f\"{i[1]} to {i[0]}\")\n",
    "\n",
    "    print(\"\\b\")\n",
    "    print(\n",
    "        \"Infra Flake:\",\n",
    "    )\n",
    "    for infr in results[\"infra_flake\"]:\n",
    "        print(f\"Test number {infr[1]} had an infra flake at {infr[0]}\")\n",
    "\n",
    "    print(\"\\b\")\n",
    "    print(\"Install Flake:\")\n",
    "    for inst in results[\"install_flake\"]:\n",
    "        print(f\"An install flake started on {inst}\")\n",
    "\n",
    "    print(\"\\b\")\n",
    "    print(\"New Test Failures:\")\n",
    "    for ntf in results[\"new_test_failure\"]:\n",
    "        print(f\"Test number {ntf[0]} had new test failures at:\")\n",
    "        for i in ntf[1]:\n",
    "            print(f\"{i[1]} to {i[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to re-map the values so that the output of the convolution will be more interpretable.\n",
    "def normalize(x):\n",
    "    if x == 1:\n",
    "        return 1\n",
    "    if x == 12:\n",
    "        return -1\n",
    "    if x == 0:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "def run_length_encode(x):\n",
    "    \"\"\"run length encoding\"\"\"\n",
    "\n",
    "    rle = []\n",
    "    count = 1\n",
    "    for i, j in enumerate(x):\n",
    "        key = j\n",
    "        if i == len(x) - 1:\n",
    "            rle.append((key, count, i))\n",
    "            break\n",
    "        if key == x[i + 1]:\n",
    "            count += 1\n",
    "        else:\n",
    "            rle.append((key, count, i))\n",
    "            count = 1\n",
    "    return rle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
